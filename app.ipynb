{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0912995f-58a4-4ead-b99f-df7fd6eccc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7b9a6-a578-4840-8efc-bd04a08f0344",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Define the sigmoid activation function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7dee6-f553-4953-9749-215fe0eeea66",
   "metadata": {},
   "source": [
    "**What is the Sigmoid Function?**\n",
    "The sigmoid function maps any real-valued number to a value between 0 and 1. Its formula is:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "x is the input (can be a scalar, vector, or matrix)\n",
    "\n",
    "e is Euler’s number (approximately 2.718)\n",
    "\n",
    "$ \\sigma(x)$ is the output, always between 0 and 1\n",
    "\n",
    "### [Sigmoid Function Graph - Math-Deep-Dives](https://github.com/progressivepull/Math-Deep-Dives/blob/main/sigmoid_function-graph.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fd1b417-895d-4217-9709-faaefa6ea9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ddc4a-5965-489d-bdbc-c5a8297307ed",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Define the structure of the neural network</span> \n",
    "* Input layer: X1, X2, X3\n",
    "* Hidden layers: Two layers with weights and biases\n",
    "* Output layer: Single output node\n",
    "\n",
    "| Layer          | Function                         |\n",
    "|----------------|----------------------------------|\n",
    "| Input Layer    | Receives raw data                |\n",
    "| Hidden Layers  | Extract and combine features     |\n",
    "| Output Layer   | Produces the final prediction/result |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccde458-44d0-4313-9af3-47f5b0354445",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Input Layer</span> \n",
    "\n",
    "### **Breakdown:**\n",
    "* inputs: This is a NumPy array containing three values: [1, 0, 1].\n",
    "* Each position corresponds to a variable:\n",
    "    - X1 = Good waves: The first value (1) means yes, the waves are good.\n",
    "    - X2 = Crowded beaches: The second value (0) means no, the beach are not empty of people.\n",
    "    - X3 = Shark-free zone: The third value (1) means yes, the area is free of sharks.\n",
    "* The values:\n",
    "    - 1 = Yes (the condition is true)\n",
    "    - 0 = No (the condition is false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a24cada7-1ce8-44a0-8f54-4f8ad09cb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([1, 0, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d5e7e-dd55-40e0-9261-553c20b28644",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Weights and Biases</span>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9bb58-07b6-48dc-9db5-d1a8a3d5ab45",
   "metadata": {},
   "source": [
    "# Weights for the first hidden layer \n",
    "* 3 inputs, 4 neurons in the first hidden layer\n",
    "* 4 neurons in the first hidden layer: Each neuron in this layer receives input from all three input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18a178fe-e57e-491c-82b5-6aa5e5edaa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_hidden1 = np.array([\n",
    "    [0.5, 0.2, 0.1, 0.4],  # Weights for X1\n",
    "    [0.3, 0.8, 0.5, 0.7],  # Weights for X2\n",
    "    [0.6, 0.1, 0.3, 0.9]   # Weights for X3\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286327ca-26a0-4d98-8544-29d3049119d9",
   "metadata": {},
   "source": [
    "* This is a 3x4 matrix (3 rows for inputs, 4 columns for neurons).\n",
    "\n",
    "### How to Read the Matrix:\n",
    "* Rows: Each row corresponds to an input variable (X1, X2, X3).\n",
    "* Columns: Each column corresponds to a neuron in the first hidden layer (Neuron 1, Neuron 2, Neuron 3, Neuron 4).\n",
    "\n",
    "|        | Neuron 1 | Neuron 2 | Neuron 3 | Neuron 4 |\n",
    "|--------|----------|----------|----------|----------|\n",
    "| **X1** |   0.5    |   0.2    |   0.1    |   0.4    |\n",
    "| **X2** |   0.3    |   0.8    |   0.5    |   0.7    |\n",
    "| **X3** |   0.6    |   0.1    |   0.3    |   0.9    |\n",
    "\n",
    "### What does this mean?\n",
    "* For Neuron 1:      \n",
    "    - Weight from X1: 0.5\n",
    "    - Weight from X2: 0.3\n",
    "    - Weight from X3: 0.6\n",
    "* For Neuron 2:     \n",
    "    - Weight from X1: 0.2\n",
    "    - Weight from X2: 0.8\n",
    "    - Weight from X3: 0.1\n",
    "* ... and so on for each neuron.\n",
    "\n",
    "### In Calculation:\n",
    "When you multiply your input vector by this weight matrix (plus a bias, if present), you get the raw input (before activation) for each neuron in the hidden layer. For example, if your input is [1, 0, 1], the calculation for Neuron 1 would be:\n",
    "\n",
    "```\n",
    "(1 * 0.5) + (0 * 0.3) + (1 * 0.6) = 0.5 + 0 + 0.6 = 1.1\n",
    "```\n",
    "Repeat for each neuron using the appropriate column.\n",
    "\n",
    "### Summary:\n",
    "***weights_hidden1*** defines how strongly each input affects each neuron in the first hidden layer. Each element in the matrix is a “weight” that will be multiplied with its corresponding input value during the forward pass of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e5c13-5d8e-41cc-a86d-467aad0e695d",
   "metadata": {},
   "source": [
    "# Biases for the first hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17819f1-a6fd-4d3e-beb5-2b54e5d97daa",
   "metadata": {},
   "source": [
    "Biases are additional parameters in neural networks, just like weights. Each neuron in a neural network (except sometimes for the input layer) typically has its own bias.\n",
    "\n",
    "### Why are biases needed?\n",
    "* **Shift the Activation:** The bias allows the activation function of a neuron to be shifted to the left or right, which makes the network more flexible and able to fit the data better.\n",
    "* **Without Bias:** If there were no bias, all activations would always pass through the origin (0,0), limiting the network’s ability to model real-world data.\n",
    "\n",
    "### Mathematical Representation\n",
    "For a single neuron:\n",
    "\n",
    "```\n",
    "output = activation(weighted_sum + bias)\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- **weighted_sum** = w₁x₁ + w₂x₂ + ... + wₙxₙ\n",
    "- **bias** is a constant added to the weighted sum\n",
    "- **activation** is a function (like sigmoid, ReLU, etc.)\n",
    "\n",
    "### Example\n",
    "Suppose you have a neuron with 3 inputs and a bias:\n",
    "\n",
    "* Inputs: [x1, x2, x3]\n",
    "* Weights: [w1, w2, w3]\n",
    "* Bias: b\n",
    "* \n",
    "The neuron computes:\n",
    "\n",
    "```\n",
    "output = activation(w1*x1 + w2*x2 + w3*x3 + b)\n",
    "```\n",
    "### Visualization\n",
    "Think of bias as the intercept in a linear equation (y = mx + b), where b is the intercept (bias). It allows the line (or the activation function in a neuron) to move up/down independently of the input.\n",
    "\n",
    "### Summary\n",
    "* Biases are trainable parameters in neural networks.\n",
    "* They allow neurons to fit the data better by shifting the activation function.\n",
    "* Every neuron typically has its own bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab7ebe2-3ada-40ef-b988-cf38fcb4a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_hidden1 = np.array([0.1, 0.2, 0.3, 0.1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3b824-7b4d-4fc2-b041-0f62c6f3f38a",
   "metadata": {},
   "source": [
    "# Weights for the second hidden layer \n",
    "(4 inputs, 3 neurons in the second hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984e5400-2d39-4dcc-a40a-39043a1df4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_hidden2 = np.array([\n",
    "    [0.5, 0.3, 0.6],\n",
    "    [0.8, 0.2, 0.9],\n",
    "    [0.4, 0.7, 0.5],\n",
    "    [0.6, 0.1, 0.3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179dcfa-48bf-42ad-ad0f-321ee37d47e7",
   "metadata": {},
   "source": [
    "# Biases for the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f7d919-92c5-4845-ab13-5d81ee9dc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_hidden2 = np.array([0.2, 0.1, 0.3])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f2132-c1ca-41be-8798-7f739a5e0a83",
   "metadata": {},
   "source": [
    "# Weights for the output layer \n",
    "(3 inputs, 1 neuron in the output layer)\n",
    "\n",
    "The **output layer** is the final layer in a neural network. Its main purpose is to produce the network’s prediction or result, based on the information processed by the previous layers (input and hidden layers).\n",
    "\n",
    "### Key Points:\n",
    "* **Final Stage:** It takes the outputs from the last hidden layer, processes them (using weights, biases, and an activation function), and produces the final result.\n",
    "* **Shape:** The number of neurons in the output layer depends on the kind of problem:\n",
    "    - **Regression:** 1 neuron (for a single predicted value, e.g., house price).\n",
    "    - **Binary Classification:** 1 neuron (outputting a value between 0 and 1, e.g., probability of “yes” or “no”).\n",
    "    - **Multi-class Classification:** 1 neuron per class (e.g., for 3 classes, 3 neurons).\n",
    "* **Activation Function:** The output layer often uses a special activation function to shape the results:\n",
    "    - **Regression:** Linear (no activation or just return the value).\n",
    "    - **Binary Classification:** Sigmoid (to return a probability between 0 and 1).\n",
    "    - **Multi-class Classification:** Softmax (to return probabilities for each class).\n",
    "\n",
    "### Example\n",
    "Suppose you have a neural network for classifying whether to \"Go Surfing\" or \"Stay Home\":\n",
    "\n",
    "* The output layer might have 1 neuron (for binary decision).\n",
    "* It takes the values from the last hidden layer, applies weights and bias, then passes the result through a sigmoid function.\n",
    "* The neuron outputs a value between 0 and 1:        \n",
    "    - Closer to 1 = “Go Surfing!”\n",
    "    - Closer to 0 = “Stay Home.”\n",
    "\n",
    "### Mathematically:\n",
    "\n",
    "```\n",
    "output = activation(weighted_sum_from_hidden + bias)\n",
    "```\n",
    "\n",
    "### In short:\n",
    "The output layer is where your neural network “speaks”—it gives you the answer to whatever question you set it up to solve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "391ea18e-c224-41ba-bcbd-f41b47a21cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_output = np.array([0.7, 0.5, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80ddaa-9bed-4054-8b37-ecfa23fe4ca0",
   "metadata": {},
   "source": [
    "# Bias for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5676ce-2d32-4940-9afa-8dc3a44c5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_output = -0.5  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d102aa-59d3-4b4c-9366-e5d0ff5dc570",
   "metadata": {},
   "source": [
    "# Forward pass through the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf65289-77ae-4c29-a0df-1e423924d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1_input = np.dot(inputs, weights_hidden1) + bias_hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b34bf5-e0d3-4c20-8173-9e30c77c4779",
   "metadata": {},
   "source": [
    " # Apply activation function\n",
    "\n",
    "### Reasons to Use the Sigmoid Function:\n",
    "\n",
    "1. Non-linearity\n",
    "\n",
    "     - The sigmoid introduces non-linearity to the model, allowing the neural network to learn more complex patterns beyond just straight lines.\n",
    "\n",
    "2. Output Range (0 to 1)\n",
    "\n",
    "    - The sigmoid function squashes any input value to a range between **0 and 1**.\n",
    "    - This is especially useful when you want the output to represent a probability **(e.g., probability of \"yes\" or \"no\")**.\n",
    "      \n",
    "3. Probability Interpretation\n",
    "\n",
    "    - Because the output is between 0 and 1, it can be directly interpreted as the probability of belonging to a certain class (e.g., \"Go Surfing\" = 1, \"Don't Go Surfing\" = 0).\n",
    "    - \n",
    "4. Smooth Gradient\n",
    "\n",
    "   \n",
    "    - The function is smooth and differentiable, which helps with the optimization process during training (using gradient descent).\n",
    "\n",
    "### Example Usage\n",
    "Suppose the output of your neuron (before activation) is 2.0:\n",
    "\n",
    "$$\n",
    "\\text{sigmoid}(2.0) = \\frac{1}{1 + e^{-2.0}} \\approx 0.88\n",
    "$$\n",
    "\n",
    "This means the model predicts an **88% probability** for the positive class. \n",
    "\n",
    "### Common Usage Scenarios\n",
    "* Output layer of binary classification networks\n",
    "* Logistic regression models\n",
    "* Sometimes in hidden layers (though ReLU is more popular there now)\n",
    "\n",
    "### Summary:\n",
    "The sigmoid activation function is applied to constrain outputs between 0 and 1, making them interpretable as probabilities and enabling neural networks to learn non-linear patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b498ce7-8e66-415a-aedf-474747310669",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1_output = sigmoid(hidden1_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19181ee-ee74-4e19-af73-751d0bb085d7",
   "metadata": {},
   "source": [
    "# Forward pass through the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b02aeee3-b1e6-4690-9b2d-2f00970615bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden2_input = np.dot(hidden1_output, weights_hidden2) + bias_hidden2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc107f3-2a8b-4ad4-8088-742b8636a8e6",
   "metadata": {},
   "source": [
    " # Apply activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4738827a-a3f2-4636-b062-31e3caa8a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden2_output = sigmoid(hidden2_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aa954-9f75-45d3-9cfa-6f4484cab20a",
   "metadata": {},
   "source": [
    "# Forward pass through the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56e12306-30b2-4959-93ca-1527a3cee4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_input = np.dot(hidden2_output, weights_output) + bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0bbfdc-2cde-4095-8a1a-1073b385c9e8",
   "metadata": {},
   "source": [
    "# Apply activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42afd90c-d4b3-4034-ad34-a17aba1a0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sigmoid(output_input)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284a4f0-0865-4e61-ad97-d86c6418a7ab",
   "metadata": {},
   "source": [
    "# Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c2eff1-a532-4314-ad17-dbbc635679a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 1 (Decision: We’re going surfing!)\n"
     ]
    }
   ],
   "source": [
    "if output > 0.5:  # Using 0.5 as the threshold\n",
    "    print(f\"Output: 1 (Decision: We’re going surfing!)\")\n",
    "else:\n",
    "    print(f\"Output: 0 (Decision: No surfing today.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669e871-e5ea-47d0-8329-cb92cb39cfd5",
   "metadata": {},
   "source": [
    "# Display the computed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ae0c8f-b0d4-46a5-a650-d6fcc45c5b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output value (Y_hat): 0.762182530316983\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output value (Y_hat): {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b3cad-9d49-4afc-b8da-eaf37ab75999",
   "metadata": {},
   "source": [
    "# [Go Surfing Context](./README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
